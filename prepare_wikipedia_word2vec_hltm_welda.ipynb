{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.parsing.preprocessing import preprocess_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through how the data was gathered and preprocessed for this project. This can be used as a guide on how other data sets or word vector models should be substituted in for these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch `20newsgroups` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 62.9 ms, total: 13.4 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_preprocessed = preprocess_documents(newsgroups.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = np.array([np.array(doc) for doc in text_preprocessed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_length(x):\n",
    "    return len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "elv = np.vectorize(element_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = elv(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_del = np.where(doc_lengths == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = np.delete(arr=text_preprocessed, obj=idx_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = np.array(newsgroups.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = np.delete(arr=corpus_raw, obj=idx_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = newsgroups.data\n",
    "\n",
    "with open('hltm_welda/model/data/newsgroups_raw_data.pickle', 'wb') as f:\n",
    "    pickle.dump(obj=corpus_raw, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hltm_welda/model/data/newsgroups_preprocessed_data.pickle', 'wb') as f:\n",
    "    pickle.dump(obj=text_preprocessed, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Google's `word2vec` model trained on Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google's word2vec model pretrained on Wikipedia can be found [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit). Be sure to unzip the file before loading it. Be sure to use the correct filepath to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_300d = KeyedVectors.load_word2vec_format(\n",
    "    fname='~/Downloads/GoogleNews-vectors-negative300.bin',\n",
    "    binary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab_sorted = sorted(list(w2v_300d.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_300d_vocab_sorted_vects = np.zeros(shape=(len(w2v_vocab_sorted), 300), dtype=np.float64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.4 s, sys: 8.11 s, total: 31.5 s\n",
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for index, word in enumerate(w2v_vocab_sorted):\n",
    "    w2v_300d_vocab_sorted_vects[index] = w2v_300d.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(\n",
    "    n_components=2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 1min 4s, total: 2min 35s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "w2v_pca = pca.fit_transform(X=w2v_300d_vocab_sorted_vects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.12 s, sys: 1.97 s, total: 5.1 s\n",
      "Wall time: 6.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "w2v_pca_dict = {\n",
    "    w2v_vocab_sorted[index]: w2v_pca[index]\n",
    "    for index in range(len(w2v_vocab_sorted))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.23 s, sys: 57.4 ms, total: 5.29 s\n",
      "Wall time: 5.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tree = cKDTree(\n",
    "    data=w2v_pca,\n",
    "    leafsize=16,\n",
    "    compact_nodes=True,\n",
    "    balanced_tree=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_id2token = {\n",
    "    index: word\n",
    "    for index, word\n",
    "    in enumerate(w2v_300d.vocab.keys())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_token2id = {\n",
    "    word: index\n",
    "    for index, word\n",
    "    in w2v_id2token.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to `hl_welda_tmp` project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hltm_welda/model/data/w2v_wikipedia_pca_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(obj=w2v_pca_dict, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hltm_welda/model/data/cKDTree_w2v_wikipedia_pca.pickle', 'wb') as f:\n",
    "    pickle.dump(obj=tree, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hltm_welda/model/data/w2v_wikipedia_id2token.pickle', 'wb') as f:\n",
    "    pickle.dump(obj=w2v_id2token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hltm_welda/model/data/w2v_wikipedia_token2id.pickle', 'wb') as f:\n",
    "    pickle.dump(obj=w2v_token2id, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
